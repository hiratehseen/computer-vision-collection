{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8ebF5OiRFKDz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoqtILwb4w-8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf1 = pd.DataFrame()\\npath_to_file = \"/content/drive/MyDrive/nabdi/data1.csv\"\\nchunk_size= 1000\\nfor df in pd.read_csv(path_to_file,chunksize=chunk_size):\\n  df1 = pd.concat([df1,df])\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df1 = pd.DataFrame()\n",
    "path_to_file = \"/content/drive/MyDrive/nabdi/data1.csv\"\n",
    "chunk_size= 1000\n",
    "for df in pd.read_csv(path_to_file,chunksize=chunk_size):\n",
    "  df1 = pd.concat([df1,df])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VzddCRus69iU"
   },
   "outputs": [],
   "source": [
    "# # read the dataset\n",
    "\n",
    "# path_to_file = \"X columns.csv\"\n",
    "# # read the data with nrows = 500 and append the data to the dataframe with a loop\n",
    "# df = pd.DataFrame()\n",
    "# chunk_size= 1000\n",
    "# for df1 in pd.read_csv(path_to_file,chunksize=chunk_size):\n",
    "#     print(df1)\n",
    "#     break\n",
    "# #     df = pd.concat([df,df1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX columns.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:581\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1252\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1255\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:805\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:883\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1044\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"X columns.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgSdhdl49itk"
   },
   "outputs": [],
   "source": [
    "# df2 = pd.read_csv(\"Y columns.csv\", nrows=1000)\n",
    "df2 = pd.read_csv(\"Y columns.csv\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIjpch9Rfw2V"
   },
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-5QH5s7r8sB"
   },
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elL6fM2_spfQ"
   },
   "outputs": [],
   "source": [
    "df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h225CiIw78fv"
   },
   "outputs": [],
   "source": [
    "X = np.array(df.iloc[0:1000 , 1: ])\n",
    "print(X)\n",
    "\n",
    "Y1 = np.array(df2.iloc[0:1000, 1:2])\n",
    "Y2 = np.array(df2.iloc[0:1000, 2:3])\n",
    "Y3 = np.array(df2.iloc[0:1000, 3:4])\n",
    "Y4 = np.array(df2.iloc[0:1000, 4:5])\n",
    "Y5 = np.array(df2.iloc[0:1000, 5:6])\n",
    "Y6 = np.array(df2.iloc[0:1000, 6:7])\n",
    "Y7 = np.array(df2.iloc[0:1000, 7:8])\n",
    "Y8 = np.array(df2.iloc[0:1000, 8:9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnu1itauCC0s"
   },
   "outputs": [],
   "source": [
    "print(X.ndim)\n",
    "print(Y1.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHt5a_43xLk3"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y1_train, Y1_test = train_test_split(X,Y1,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y2_train, Y2_test = train_test_split(X,Y2,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y3_train, Y3_test = train_test_split(X,Y3,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y4_train, Y4_test = train_test_split(X,Y4,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y5_train, Y5_test = train_test_split(X,Y5,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y6_train, Y6_test = train_test_split(X,Y6,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y7_train, Y7_test = train_test_split(X,Y7,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y8_train, Y8_test = train_test_split(X,Y8,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZdFv9EypyEtK"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y1_train.shape)\n",
    "print(Y1_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPcUvbYzDyZ8"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPP1MflwzB2H"
   },
   "source": [
    "#**Normalizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avfSuB9xzBVZ"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')         \n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "Y1_train = np.reshape(Y1_train,(Y1_train.shape[0],))\n",
    "Y2_train = np.reshape(Y2_train,(Y2_train.shape[0],))\n",
    "Y3_train = np.reshape(Y3_train,(Y3_train.shape[0],))\n",
    "Y4_train = np.reshape(Y4_train,(Y4_train.shape[0],))\n",
    "Y5_train = np.reshape(Y5_train,(Y5_train.shape[0],))\n",
    "Y6_train = np.reshape(Y6_train,(Y6_train.shape[0],))\n",
    "Y7_train = np.reshape(Y7_train,(Y7_train.shape[0],))\n",
    "Y8_train = np.reshape(Y8_train,(Y8_train.shape[0],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gm-JpGmMxMyy"
   },
   "source": [
    "#**Model definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DtL0ZM2YxMRb"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(64, activation='relu',input_shape=(X_train.shape[1],)))\n",
    "  model.add(layers.Dense(64, activation='relu'))\n",
    "  model.add(layers.Dense(1))\n",
    "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYUSX1eOqVq5"
   },
   "source": [
    "#**K-fold validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxAmTUvaeAs3"
   },
   "source": [
    "#**Model no 01**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXjibksrz4l2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 10\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y1_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y1_train = np.concatenate(\n",
    "      [Y1_train[:i * num_val_samples],\n",
    "      Y1_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model = build_model()\n",
    "  print(\"shapes of x partial\",partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y1_train.shape)\n",
    "  model.fit(partial_X_train, partial_Y1_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTkU0LiM-oH1"
   },
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score = model.evaluate(X_test, Y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM4Fhid-4bSL"
   },
   "source": [
    "#**Saving the validation logs at each fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmoJudak4KDZ"
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y1_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  partial_Y1_train = np.concatenate(\n",
    "        [Y1_train[:i * num_val_samples],\n",
    "         Y1_train[(i + 1) * num_val_samples:]],\n",
    "        axis=0)\n",
    "  model = build_model()\n",
    "  history = model.fit(partial_X_train, partial_Y1_train,\n",
    "                      validation_data=(val_data, val_targets),\n",
    "                      epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  mae_history = history.history['mae']\n",
    "  all_mae_histories.append(mae_history)\n",
    "\n",
    "# You can then compute the average of the per-epoch MAE scores for all folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DtpJWY26UkP"
   },
   "source": [
    "#**Mean K-fold validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7OZGnym4LkR"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_mae_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SocJ7p7Z6i7R"
   },
   "source": [
    "#**Plotting validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KpadXNa4LrC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8t5SMRyA5yU"
   },
   "source": [
    "#**Training the final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lOJgglAA7pa"
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "model.fit(X_train, Y1_train, epochs=50, batch_size=16, verbose=1)\n",
    "test_mse_score, test_mae_score = model.evaluate(X_test, Y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpAQJzA-n0M_"
   },
   "source": [
    "#**Save the model_1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GihHEU6JmYKq"
   },
   "outputs": [],
   "source": [
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model should be saved to HDF5.\n",
    "\n",
    "model.save('model_1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2igrtqAiuzHC"
   },
   "source": [
    "#**load the model_1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jnV0MV2u4e6"
   },
   "outputs": [],
   "source": [
    "# Now, recreate the model from that file:\n",
    "import tensorflow as tf\n",
    "\n",
    "new_model = tf.keras.models.load_model('model_1.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft32NhSYeFk_"
   },
   "source": [
    "#**Model no 02**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79SSLPV7czDh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 10\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y2_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y2_train = np.concatenate(\n",
    "      [Y2_train[:i * num_val_samples],\n",
    "      Y2_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_2 = build_model()\n",
    "  print(\"shapes of x partial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y2_train.shape)\n",
    "  model_2.fit(partial_X_train, partial_Y2_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_2.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdYdqKWZokxE"
   },
   "source": [
    "#**Save the model_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZS1mGenomvj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVJ6jEAueLtO"
   },
   "source": [
    "#**Model no 03**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y10VGFv9c0O4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 10\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y3_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y3_train = np.concatenate(\n",
    "      [Y3_train[:i * num_val_samples],\n",
    "      Y3_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_3 = build_model()\n",
    "  print(\"shapes of x patial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y3_train.shape)\n",
    "  model_3.fit(partial_X_train, partial_Y3_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_3.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkmgsKbHow6z"
   },
   "source": [
    "#**Save the model_3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyi3-Zcuoxsk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XRLWW4febLm"
   },
   "source": [
    "#**Model no 04**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fsEwp90vc0Zv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 10\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y4_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y4_train = np.concatenate(\n",
    "      [Y4_train[:i * num_val_samples],\n",
    "      Y4_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_4 = build_model()\n",
    "  print(\"shapes of x patial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y4_train.shape)\n",
    "  model_4.fit(partial_X_train, partial_Y4_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_4.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFNC5VUSoyxj"
   },
   "source": [
    "#**Save the model_4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Y1OYRGBozJz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfgDdgx6edx8"
   },
   "source": [
    "#**Model no 05**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3TmvVjGc0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 10\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y5_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y5_train = np.concatenate(\n",
    "      [Y5_train[:i * num_val_samples],\n",
    "      Y5_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_5 = build_model()\n",
    "  print(\"shapes of x patial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y5_train.shape)\n",
    "  model_5.fit(partial_X_train, partial_Y5_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_5.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xB5QRJUho0AL"
   },
   "source": [
    "#**Save the model_5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYyuItIdo0bM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMf1ckV4ehbu"
   },
   "source": [
    "#**Model no 06**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2x1furtVc0kv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 10\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y6_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y6_train = np.concatenate(\n",
    "      [Y6_train[:i * num_val_samples],\n",
    "      Y6_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_6 = build_model()\n",
    "  print(\"shapes of x patial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y6_train.shape)\n",
    "  model_6.fit(partial_X_train, partial_Y6_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_6.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKG_B_a-o1Vz"
   },
   "source": [
    "#**Save the model_6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-AP4rz4o1pr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOOCGkuaekdN"
   },
   "source": [
    "#**Model no 07**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hM6Zdw-Gc01v"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y7_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y7_train = np.concatenate(\n",
    "      [Y7_train[:i * num_val_samples],\n",
    "      Y7_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_7 = build_model()\n",
    "  print(\"shapes of x patial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y7_train.shape)\n",
    "  model_7.fit(partial_X_train, partial_Y7_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_7.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGZwvtv5o2d7"
   },
   "source": [
    "#**Save the model_7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feFI4wFro3Ar"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LbcXv9DemV0"
   },
   "source": [
    "#**Model no 08**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2tuxLJ9ep88"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y8_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y8_train = np.concatenate(\n",
    "      [Y8_train[:i * num_val_samples],\n",
    "      Y8_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_8 = build_model()\n",
    "  print(\"shapes of x patial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y8_train.shape)\n",
    "  model_8.fit(partial_X_train, partial_Y8_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_8.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jegc5_cso354"
   },
   "source": [
    "#**Save the model_8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNC6qk2ro4f7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Tgh4FpSog_A"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Prediction.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
