{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QtkqrPYiiql"
   },
   "source": [
    "#**Importing Essential libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "8ebF5OiRFKDz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19zHIM061_jw"
   },
   "outputs": [],
   "source": [
    "#reading csv file to dataframe\n",
    "\n",
    "path_to_file = \"/content/drive/MyDrive/Farhan/data.csv\"\n",
    "# read the data with 1000 and append the data to the dataframe with a loop\n",
    "df = pd.DataFrame()\n",
    "chunk_size= 1000\n",
    "for df1 in pd.read_csv(path_to_file, chunksize=chunk_size):\n",
    "    df = pd.concat([df,df1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uEloj-ewSRi"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgSdhdl49itk"
   },
   "outputs": [],
   "source": [
    "#reaading csv file of labels/outputs\n",
    "\n",
    "df2 = pd.read_csv(\"/content/drive/MyDrive/Farhan/out1.csv\")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGSJJ67e188Y"
   },
   "outputs": [],
   "source": [
    "#dropping unnamed column from both dataframes\n",
    "\n",
    "df1 = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "df2 = df2.drop([\"Unnamed: 0\"], axis =1)\n",
    "df2.columns = [\"0\",\"1\",\"2\",\"3\",'4','5',\"6\",\"7\",\"8\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIjpch9Rfw2V"
   },
   "outputs": [],
   "source": [
    "#checking length of both dataframes\n",
    "\n",
    "print(len(df))\n",
    "print(len(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-5QH5s7r8sB"
   },
   "outputs": [],
   "source": [
    "#checking shape of both dataframes\n",
    "\n",
    "print(df.shape)\n",
    "print(df2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4HIrHWhbvFy"
   },
   "outputs": [],
   "source": [
    "# This will Randomly permute a sequence of both dataframes \n",
    "\n",
    "idx = np.random.permutation(df1.index)\n",
    "df1 = df1.reindex(idx)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-vNiyFqJ2eg"
   },
   "outputs": [],
   "source": [
    "df2 = df2.reindex(idx)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h225CiIw78fv"
   },
   "outputs": [],
   "source": [
    "# Slicing the all rows and columns of input dataframe\n",
    "\n",
    "X = np.array(df1.iloc[: , : ])\n",
    "\n",
    "#slicing the data of output dataframe for that model which have to be trained.\n",
    "\n",
    "Y1 = np.array(df2.iloc[:, 0:1])\n",
    "Y2 = np.array(df2.iloc[:, 1:2])\n",
    "Y3 = np.array(df2.iloc[:, 2:3])\n",
    "Y4 = np.array(df2.iloc[:, 3:4])\n",
    "Y5 = np.array(df2.iloc[:, 4:5])\n",
    "Y6 = np.array(df2.iloc[:, 5:6])\n",
    "Y7 = np.array(df2.iloc[:, 6:7])\n",
    "Y8 = np.array(df2.iloc[:, 7:8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drypaaS3EIdj"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUbvJKx_Ermh"
   },
   "outputs": [],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xnu1itauCC0s"
   },
   "outputs": [],
   "source": [
    "# checking the dimensions of both dataframes\n",
    "\n",
    "print(X.ndim)\n",
    "\n",
    "print(Y1.ndim)\n",
    "print(Y2.ndim)\n",
    "print(Y3.ndim)\n",
    "print(Y4.ndim)\n",
    "print(Y5.ndim)\n",
    "print(Y6.ndim)\n",
    "print(Y7.ndim)\n",
    "print(Y8.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7WefUXbflj-3"
   },
   "source": [
    "#**Splitting Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHt5a_43xLk3"
   },
   "outputs": [],
   "source": [
    "#importing train_test_split module to break data into an train and test data sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Splitting data for that models which have to be trained.\n",
    "\n",
    "X_train, X_test, Y1_train, Y1_test = train_test_split(X,Y1,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y2_train, Y2_test = train_test_split(X,Y2,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y3_train, Y3_test = train_test_split(X,Y3,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y4_train, Y4_test = train_test_split(X,Y4,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y5_train, Y5_test = train_test_split(X,Y5,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y6_train, Y6_test = train_test_split(X,Y6,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y7_train, Y7_test = train_test_split(X,Y7,test_size=0.2, random_state=42)\n",
    "X_train, X_test, Y8_train, Y8_test = train_test_split(X,Y8,test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upxbXG951WQg"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPcUvbYzDyZ8"
   },
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCzLiVLgjT6W"
   },
   "outputs": [],
   "source": [
    "  #X_train = X_train.shape[0]\n",
    "  #X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPP1MflwzB2H"
   },
   "source": [
    "#**Normalizing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "avfSuB9xzBVZ"
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')         \n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# We will normalize data of that model which has to be trained.\n",
    "\n",
    "Y1_train = np.reshape(Y1_train,(Y1_train.shape[0],))\n",
    "Y2_train = np.reshape(Y2_train,(Y2_train.shape[0],))\n",
    "Y3_train = np.reshape(Y3_train,(Y3_train.shape[0],))\n",
    "Y4_train = np.reshape(Y4_train,(Y4_train.shape[0],))\n",
    "Y5_train = np.reshape(Y5_train,(Y5_train.shape[0],))\n",
    "Y6_train = np.reshape(Y6_train,(Y6_train.shape[0],))\n",
    "Y7_train = np.reshape(Y7_train,(Y7_train.shape[0],))\n",
    "Y8_train = np.reshape(Y8_train,(Y8_train.shape[0],))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnndm-Fljd7i"
   },
   "source": [
    "#**Instantiating a small convnet**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_toBl0hpjhE7"
   },
   "source": [
    "**Model 1 CNN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CO5SLdRTjrfK"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model_1 = models.Sequential()\n",
    "model_1.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(370,370,1)))\n",
    "model_1.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_1.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_1.add(layers.MaxPooling2D((2, 2)))\n",
    "model_1.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "btGfF93PkR1R"
   },
   "source": [
    "**Model 2 CNN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcbNPsH5kTzA"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model_2 = models.Sequential()\n",
    "model_2.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(370,370,1)))\n",
    "model_2.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_2.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_2.add(layers.MaxPooling2D((2, 2)))\n",
    "model_2.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6hZrgcHkj0lE"
   },
   "source": [
    "**Model 3 CNN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bwUjo8Njsfy"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model_3 = models.Sequential()\n",
    "model_3.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(370,370,1)))\n",
    "model_3.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_3.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_3.add(layers.MaxPooling2D((2, 2)))\n",
    "model_3.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E9erfUVZj2Cq"
   },
   "source": [
    "**Model 4 CNN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_7QeiNvjsiR"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model_4 = models.Sequential()\n",
    "model_4.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(370,370,1)))\n",
    "model_4.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_4.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_4.add(layers.MaxPooling2D((2, 2)))\n",
    "model_4.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FhTb0og2j28b"
   },
   "source": [
    "**Model 5 CNN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MmUT3iIxjsks"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model_5 = models.Sequential()\n",
    "model_5.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(370,370,1)))\n",
    "model_5.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_5.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_5.add(layers.MaxPooling2D((2, 2)))\n",
    "model_5.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksthuTWMj3-p"
   },
   "source": [
    "**Model 6 CNN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rc1sIHVLjsnr"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model_6 = models.Sequential()\n",
    "model_6.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(370,370,1)))\n",
    "model_6.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_6.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_6.add(layers.MaxPooling2D((2, 2)))\n",
    "model_6.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xpf1nNZj5LJ"
   },
   "source": [
    "**Model 7 CNN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oQhVaAtjspw"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model_7 = models.Sequential()\n",
    "model_7.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(370,370,1)))\n",
    "model_7.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_7.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_7.add(layers.MaxPooling2D((2, 2)))\n",
    "model_7.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBwqHa13j566"
   },
   "source": [
    "**Model 8 CNN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_kiuPJ2jssC"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "\n",
    "model_8 = models.Sequential()\n",
    "model_8.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(370,370,1)))\n",
    "model_8.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model_8.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model_8.add(layers.MaxPooling2D((2, 2)))\n",
    "model_8.add(layers.Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDIkKO_RrI94"
   },
   "source": [
    "#**Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yj0yoepWkiSp"
   },
   "source": [
    "**Model 1 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_61kou3jsuk"
   },
   "outputs": [],
   "source": [
    "model_1.add(layers.Flatten())\n",
    "model_1.add(layers.Dense(64, activation='relu'))\n",
    "model_1.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkgluVzHk2Wk"
   },
   "source": [
    "**Model 2 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoEASG8MjsxA"
   },
   "outputs": [],
   "source": [
    "model_2.add(layers.Flatten())\n",
    "model_2.add(layers.Dense(64, activation='relu'))\n",
    "model_2.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmWBqq7tk4T2"
   },
   "source": [
    "**Model 3 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_Eg6FO_jszZ"
   },
   "outputs": [],
   "source": [
    "model_3.add(layers.Flatten())\n",
    "model_3.add(layers.Dense(64, activation='relu'))\n",
    "model_3.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyUxxGdnk6S_"
   },
   "source": [
    "**Model 4 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqB0VYoljs18"
   },
   "outputs": [],
   "source": [
    "model_4.add(layers.Flatten())\n",
    "model_4.add(layers.Dense(64, activation='relu'))\n",
    "model_4.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTXUHDIQk83O"
   },
   "source": [
    "**Model 5 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5FD2FQZjs4U"
   },
   "outputs": [],
   "source": [
    "model_5.add(layers.Flatten())\n",
    "model_5.add(layers.Dense(64, activation='relu'))\n",
    "model_5.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WLIaiO6Gk_Rv"
   },
   "source": [
    "**Model 6 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9bP0FFvjs61"
   },
   "outputs": [],
   "source": [
    "model_6.add(layers.Flatten())\n",
    "model_6.add(layers.Dense(64, activation='relu'))\n",
    "model_6.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dOf2j9tlBa2"
   },
   "source": [
    "**Model 7 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XdWK_JHijs9e"
   },
   "outputs": [],
   "source": [
    "model_7.add(layers.Flatten())\n",
    "model_7.add(layers.Dense(64, activation='relu'))\n",
    "model_7.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QeJblsgOlD8W"
   },
   "source": [
    "**Model 8 Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQXa-BLQjs_w"
   },
   "outputs": [],
   "source": [
    "model_8.add(layers.Flatten())\n",
    "model_8.add(layers.Dense(64, activation='relu'))\n",
    "model_8.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaRSNXCUjwGy"
   },
   "source": [
    "#**Model definition 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhnUh72F5ITc"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "  model_1 = models.Sequential()\n",
    "  model_1.add(layers.Dense(64, activation='relu',input_shape=(X_train.shape[1],)))\n",
    "  model_1.add(layers.Dense(64, activation='relu'))\n",
    "  model_1.add(layers.Dense(1))\n",
    "  model_1.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "  return model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-CFl89G3TJ1"
   },
   "source": [
    "#**Model definition 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M16yehaZ3G-m"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "  model_2 = models.Sequential()\n",
    "  model_2.add(layers.Dense(64, activation='relu',input_shape=(X_train.shape[1],)))\n",
    "  model_2.add(layers.Dense(64, activation='relu'))\n",
    "  model_2.add(layers.Dense(1))\n",
    "  model_2.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "  return model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cliQBIFj3Xx9"
   },
   "source": [
    "#**Model definition 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8uCqrIUn3HF2"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "  model_3 = models.Sequential()\n",
    "  model_3.add(layers.Dense(64, activation='relu',input_shape=(X_train.shape[1],)))\n",
    "  model_3.add(layers.Dense(64, activation='relu'))\n",
    "  model_3.add(layers.Dense(1))\n",
    "  model_3.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "  return model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb9li3Q63ZBV"
   },
   "source": [
    "#**Model definition 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MoUp8Ww3NvC-"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "  model_4 = models.Sequential()\n",
    "  model_4.add(layers.Dense(64, activation='relu',input_shape=(X_train.shape[1],)))\n",
    "  model_4.add(layers.Dense(64, activation='relu'))\n",
    "  model_4.add(layers.Dense(1))\n",
    "  model_4.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "  return model_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-vS9naPH3aMv"
   },
   "source": [
    "#**Model definition 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYg2vWVzNtLn"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "  model_5 = models.Sequential()\n",
    "  model_5.add(layers.Dense(64, activation='relu',input_shape=(X_train.shape[1],)))\n",
    "  model_5.add(layers.Dense(64, activation='relu'))\n",
    "  model_5.add(layers.Dense(1))\n",
    "  model_5.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "  return model_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X1lVsh1F3bZ9"
   },
   "source": [
    "#**Model definition 6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_e32bBDYj88K"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "  model_6 = models.Sequential()\n",
    "  model_6.add(layers.Dense(64, activation='relu',input_shape=(X_train.shape[1],)))\n",
    "  model_6.add(layers.Dense(64, activation='relu'))\n",
    "  model_6.add(layers.Dense(1))\n",
    "  model_6.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "  return model_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ba18gVLM3cn_"
   },
   "source": [
    "#**Model definition 7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "henAeBKl1jvs"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "  model_7 = models.Sequential()\n",
    "  model_7.add(layers.Dense(64, activation='relu',input_shape=(X_train.shape[1],)))\n",
    "  model_7.add(layers.Dense(64, activation='relu'))\n",
    "  model_7.add(layers.Dense(1))\n",
    "  model_7.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "  return model_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXU3_SYB3d41"
   },
   "source": [
    "#**Model definition 8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzebmBnt3Hfu"
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "def build_model():\n",
    "  model_8 = models.Sequential()\n",
    "  model_8.add(layers.Dense(64, activation='relu',input_shape=(X_train.shape[1],)))\n",
    "  model_8.add(layers.Dense(64, activation='relu'))\n",
    "  model_8.add(layers.Dense(1))\n",
    "  model_8.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
    "  return model_8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxAmTUvaeAs3"
   },
   "source": [
    "#**Model no 01**\n",
    "#**K-fold validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XXjibksrz4l2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y1_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y1_train = np.concatenate(\n",
    "      [Y1_train[:i * num_val_samples],\n",
    "      Y1_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_1 = build_model()\n",
    "  print(\"shapes of x partial\",partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y1_train.shape)\n",
    "  model_1.fit(partial_X_train, partial_Y1_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_1.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cKsoeQM2c7kA"
   },
   "outputs": [],
   "source": [
    "print(all_scores)\n",
    "print(np.mean(all_scores))\n",
    "print(\"val_mse :\" , val_mse)\n",
    "print(\"val_mae :\" , val_mae)\n",
    "test_mse_score, test_mae_score = model_1.evaluate(X_test, Y1_test)\n",
    "print(\"test_mse_score :\" , test_mse_score)\n",
    "print(\"test_mae_score :\" , test_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kM4Fhid-4bSL"
   },
   "source": [
    "**Saving the validation logs at each fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmoJudak4KDZ"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y1_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  partial_Y1_train = np.concatenate(\n",
    "      [Y1_train[:i * num_val_samples],\n",
    "       Y1_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  model_1 = build_model()\n",
    "  history = model_1.fit(partial_X_train, partial_Y1_train,\n",
    "                      validation_data=(val_data, val_targets),\n",
    "                      epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  mae_history = history.history['mae']\n",
    "  all_mae_histories.append(mae_history)\n",
    "\n",
    "# You can then compute the average of the per-epoch MAE scores for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vLjYeNAuiOgX"
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DtpJWY26UkP"
   },
   "source": [
    "**Mean K-fold validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7OZGnym4LkR"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_mae_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SocJ7p7Z6i7R"
   },
   "source": [
    "**Plotting validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KpadXNa4LrC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C8t5SMRyA5yU"
   },
   "source": [
    "**Training the final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-lOJgglAA7pa"
   },
   "outputs": [],
   "source": [
    "model_1 = build_model()\n",
    "model_1.fit(X_train, Y1_train, epochs=200, batch_size=16, verbose=1)\n",
    "test_mse_score, test_mae_score = model_1.evaluate(X_test, Y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i4QLoelP0LlK"
   },
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score = model_1.evaluate(X_test, Y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rpAQJzA-n0M_"
   },
   "source": [
    "**Save the final model_1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GihHEU6JmYKq"
   },
   "outputs": [],
   "source": [
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model should be saved to HDF5.\n",
    "\n",
    "model_1.save('/content/drive/MyDrive/Farhan/Models_1/model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1LWgApCC12oc"
   },
   "outputs": [],
   "source": [
    "del model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2igrtqAiuzHC"
   },
   "source": [
    "**load the model_1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jnV0MV2u4e6"
   },
   "outputs": [],
   "source": [
    "# Now, recreate the model from that file:\n",
    "import tensorflow as tf\n",
    "\n",
    "new_model = tf.keras.models.load_model('/content/drive/MyDrive/Farhan/Models_1/model_1.h5')\n",
    "\n",
    "# Show the model architecture\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3os6NnxHUvF_"
   },
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score = new_model.evaluate(X_test, Y1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ft32NhSYeFk_"
   },
   "source": [
    "#**Model no 02**\n",
    "#**K-fold validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79SSLPV7czDh"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y2_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y2_train = np.concatenate(\n",
    "      [Y2_train[:i * num_val_samples],\n",
    "      Y2_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_2 = build_model()\n",
    "  print(\"shapes of x partial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y2_train.shape)\n",
    "  model_2.fit(partial_X_train, partial_Y2_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_2.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8Vs-P4gtYhk"
   },
   "outputs": [],
   "source": [
    "print(all_scores)\n",
    "print(np.mean(all_scores))\n",
    "print(\"val_mse :\" , val_mse)\n",
    "print(\"val_mae :\" , val_mae)\n",
    "test_mse_score, test_mae_score = model_2.evaluate(X_test, Y2_test)\n",
    "print(\"test_mse_score :\" , test_mse_score)\n",
    "print(\"test_mae_score :\" , test_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ChTpRGg3oyuF"
   },
   "source": [
    "**Saving the validation logs at each fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAkxLSqRo20s"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y2_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  partial_Y2_train = np.concatenate(\n",
    "      [Y2_train[:i * num_val_samples],\n",
    "       Y2_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  model_2 = build_model()\n",
    "  history = model_2.fit(partial_X_train, partial_Y2_train,\n",
    "                      validation_data=(val_data, val_targets),\n",
    "                      epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  mae_history = history.history['mae']\n",
    "  all_mae_histories.append(mae_history)\n",
    "\n",
    "# You can then compute the average of the per-epoch MAE scores for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Z7njuE4pC7M"
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wekDPRwKpN9u"
   },
   "source": [
    "**Mean K-fold validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbR-9Tb6pF47"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_mae_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY7goMCapS3-"
   },
   "source": [
    "**Plotting validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a34JBwf-pWGG"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBAFqK4OpbUe"
   },
   "source": [
    "**Training the final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osKV1YVVpav2"
   },
   "outputs": [],
   "source": [
    "model_2 = build_model()\n",
    "model_2.fit(X_train, Y2_train, epochs=100, batch_size=16, verbose=1)\n",
    "test_mse_score, test_mae_score = model_2.evaluate(X_test, Y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTmUdo66pw_T"
   },
   "source": [
    "**test_mse_score and test_mae_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zgtIQZNIpwm0"
   },
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score = model_2.evaluate(X_test, Y2_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdYdqKWZokxE"
   },
   "source": [
    "**Save the model_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZS1mGenomvj"
   },
   "outputs": [],
   "source": [
    "model_2.save(\"/content/drive/MyDrive/Farhan/Models_1/model_2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8s-WwWZplBM"
   },
   "source": [
    "**Del model_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQL5ZdBq3f64"
   },
   "outputs": [],
   "source": [
    "del model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVJ6jEAueLtO"
   },
   "source": [
    "#**Model no 03**\n",
    "#**K-fold validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y10VGFv9c0O4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y3_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y3_train = np.concatenate(\n",
    "      [Y3_train[:i * num_val_samples],\n",
    "      Y3_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_3 = build_model()\n",
    "  print(\"shapes of x patial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y3_train.shape)\n",
    "  model_3.fit(partial_X_train, partial_Y3_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_3.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D7B_6nuIQ2D-"
   },
   "outputs": [],
   "source": [
    "print(all_scores)\n",
    "print(np.mean(all_scores))\n",
    "print(\"val_mse :\" , val_mse)\n",
    "print(\"val_mae :\" , val_mae)\n",
    "test_mse_score, test_mae_score = model_3.evaluate(X_test, Y3_test)\n",
    "print(\"test_mse_score :\" , test_mse_score)\n",
    "print(\"test_mae_score :\" , test_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmZZCMm6qMkv"
   },
   "source": [
    "**Saving the validation logs at each fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TI2-HO_cqQv4"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y3_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  partial_Y3_train = np.concatenate(\n",
    "      [Y3_train[:i * num_val_samples],\n",
    "       Y3_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  model_3 = build_model()\n",
    "  history = model_3.fit(partial_X_train, partial_Y3_train,\n",
    "                      validation_data=(val_data, val_targets),\n",
    "                      epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  mae_history = history.history['mae']\n",
    "  all_mae_histories.append(mae_history)\n",
    "\n",
    "# You can then compute the average of the per-epoch MAE scores for all folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VefKXpRlqlkX"
   },
   "source": [
    "**Mean K-fold validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0j41COXjqmKn"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_mae_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BUVYOCVuqmcu"
   },
   "source": [
    "**Plotting validation scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bT0GSCnXqmpe"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Fn14W1ZqmxG"
   },
   "source": [
    "**Training the final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9clp7x4dqm3U"
   },
   "outputs": [],
   "source": [
    "model_3 = build_model()\n",
    "model_3.fit(X_train, Y3_train, epochs=50, batch_size=16, verbose=1)\n",
    "test_mse_score, test_mae_score = model_3.evaluate(X_test, Y3_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQLaFm2Wqm85"
   },
   "source": [
    "**test_mse_score and test_mae_score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u3Y4YIFpqnCx"
   },
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score = model_3.evaluate(X_test, Y3_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pkmgsKbHow6z"
   },
   "source": [
    "**Save the model_3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cyi3-Zcuoxsk"
   },
   "outputs": [],
   "source": [
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model should be saved to HDF5.\n",
    "\n",
    "model_3.save(\"/content/drive/MyDrive/Farhan/New Models/model_3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0V6qyAMtZHVK"
   },
   "outputs": [],
   "source": [
    "del model_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XRLWW4febLm"
   },
   "source": [
    "#**Model no 04**\n",
    "#**K-fold validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETPS7vK_AYVN"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y4_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y4_train = np.concatenate(\n",
    "      [Y4_train[:i * num_val_samples],\n",
    "      Y4_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_4 = build_model()\n",
    "  print(\"shapes of x partial\",partial_X_train.shape)\n",
    "  print(\"shapes of y partial\", partial_Y4_train.shape)\n",
    "  model_4.fit(partial_X_train, partial_Y4_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_4.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uAXN3c7_N2Wp"
   },
   "outputs": [],
   "source": [
    "print(all_scores)\n",
    "print(np.mean(all_scores))\n",
    "print(\"val_mse :\" , val_mse)\n",
    "print(\"val_mae :\" , val_mae)\n",
    "test_mse_score, test_mae_score = model_4.evaluate(X_test, Y4_test)\n",
    "print(\"test_mse_score :\" , test_mse_score)\n",
    "print(\"test_mae_score :\" , test_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCNnf23etsLn"
   },
   "source": [
    "**Saving the validation logs at each fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VW0f8hTIN4bm"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y4_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  partial_Y4_train = np.concatenate(\n",
    "      [Y4_train[:i * num_val_samples],\n",
    "       Y4_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  model_4 = build_model()\n",
    "  history = model_4.fit(partial_X_train, partial_Y4_train,\n",
    "                      validation_data=(val_data, val_targets),\n",
    "                      epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  mae_history = history.history['mae']\n",
    "  all_mae_histories.append(mae_history)\n",
    "\n",
    "# You can then compute the average of the per-epoch MAE scores for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KYK4ou-CN6hl"
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-eDUV5jIUc8"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_mae_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j47Px1o7IZrk"
   },
   "source": [
    "**Plotting using Matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYK9ZMmXOAzp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iRBko_xgIk8z"
   },
   "source": [
    "**Training the final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cse8XhWcN-72"
   },
   "outputs": [],
   "source": [
    "model_4 = build_model()\n",
    "model_4.fit(X_train, Y4_train, epochs=5, batch_size=16, verbose=1)\n",
    "test_mse_score, test_mae_score = model_4.evaluate(X_test, Y4_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p9J_63FUItMl"
   },
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XZQu22gNIU3A"
   },
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score = model_4.evaluate(X_test, Y4_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFNC5VUSoyxj"
   },
   "source": [
    "**Save the model_4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Y1OYRGBozJz"
   },
   "outputs": [],
   "source": [
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model should be saved to HDF5.\n",
    "\n",
    "model_4.save('/content/drive/MyDrive/Farhan/New Models/model_4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QqxpMlQV5xs0"
   },
   "outputs": [],
   "source": [
    "del model_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pfgDdgx6edx8"
   },
   "source": [
    "#**Model no 05**\n",
    "#**K-fold validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZ9nH5ECOGkP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y5_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y5_train = np.concatenate(\n",
    "      [Y5_train[:i * num_val_samples],\n",
    "      Y5_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_5 = build_model()\n",
    "  print(\"shapes of x patial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y5_train.shape)\n",
    "  model_5.fit(partial_X_train, partial_Y5_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_5.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvJPe59wOIKN"
   },
   "outputs": [],
   "source": [
    "print(all_scores)\n",
    "print(np.mean(all_scores))\n",
    "print(\"val_mse :\" , val_mse)\n",
    "print(\"val_mae :\" , val_mae)\n",
    "test_mse_score, test_mae_score = model_5.evaluate(X_test, Y5_test)\n",
    "print(\"test_mse_score :\" , test_mse_score)\n",
    "print(\"test_mae_score :\" , test_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15xIItD3zJZs"
   },
   "source": [
    "**Saving the validation logs at each fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYFfB59DOKPV"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y5_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  partial_Y5_train = np.concatenate(\n",
    "      [Y5_train[:i * num_val_samples],\n",
    "       Y5_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  model_5 = build_model()\n",
    "  history = model_5.fit(partial_X_train, partial_Y5_train,\n",
    "                      validation_data=(val_data, val_targets),\n",
    "                      epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  mae_history = history.history['mae']\n",
    "  all_mae_histories.append(mae_history)\n",
    "\n",
    "# You can then compute the average of the per-epoch MAE scores for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WgsKnfLPd29G"
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hs-vMbaj4rg"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_mae_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nk9zaAqZuszy"
   },
   "source": [
    "**Plotting using Matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUv5aRHDeE5O"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7JPQEMnvW54"
   },
   "source": [
    "**Training the final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HsPeit3LeE9c"
   },
   "outputs": [],
   "source": [
    "model_5 = build_model()\n",
    "model_5.fit(X_train, Y5_train, epochs=5, batch_size=16, verbose=1)\n",
    "test_mse_score, test_mae_score = model_5.evaluate(X_test, Y5_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OIBhCKumzi3T"
   },
   "source": [
    "**Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZapuKmE1eFAT"
   },
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score = model_5.evaluate(X_test, Y5_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQmj3p67zl2p"
   },
   "source": [
    "**Saving model_5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44G20lx-mfq0"
   },
   "outputs": [],
   "source": [
    "model_5.save(\"/content/drive/MyDrive/Farhan/New Models/model_5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MKX3DOLM07i"
   },
   "outputs": [],
   "source": [
    "del  model_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMf1ckV4ehbu"
   },
   "source": [
    "#**Model no 06**\n",
    "#**K-fold validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brw17mSAjVlv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y6_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y6_train = np.concatenate(\n",
    "      [Y6_train[:i * num_val_samples],\n",
    "      Y6_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_6 = build_model()\n",
    "  print(\"shapes of x patial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y6_train.shape)\n",
    "  model_6.fit(partial_X_train, partial_Y6_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_6.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y0dlEeWHjXEG"
   },
   "outputs": [],
   "source": [
    "print(all_scores)\n",
    "print(np.mean(all_scores))\n",
    "print(\"val_mse :\" , val_mse)\n",
    "print(\"val_mae :\" , val_mae)\n",
    "test_mse_score, test_mae_score = model_6.evaluate(X_test, Y6_test)\n",
    "print(\"test_mse_score :\" , test_mse_score)\n",
    "print(\"test_mae_score :\" , test_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlbJNJoQuPkd"
   },
   "source": [
    "**Saving the validation logs at each fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsoQKHOKjX3x"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y6_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  partial_Y6_train = np.concatenate(\n",
    "      [Y6_train[:i * num_val_samples],\n",
    "       Y6_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  model_6 = build_model()\n",
    "  history = model_6.fit(partial_X_train, partial_Y6_train,\n",
    "                      validation_data=(val_data, val_targets),\n",
    "                      epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  mae_history = history.history['mae']\n",
    "  all_mae_histories.append(mae_history)\n",
    "\n",
    "# You can then compute the average of the per-epoch MAE scores for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7D3MqyX3jaQv"
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ziOJ7sAjcwP"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_mae_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbhAF1atvHti"
   },
   "source": [
    "**Plotting using Matplotlib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpG9vP6twYJc"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QloBsykYvH_y"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uKZJ6lfBvIG6"
   },
   "source": [
    "**Training the final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4UoqdingjuBG"
   },
   "outputs": [],
   "source": [
    "model_6 = build_model()\n",
    "model_6.fit(X_train, Y6_train, epochs=100, batch_size=16, verbose=1)\n",
    "test_mse_score, test_mae_score = model_6.evaluate(X_test, Y6_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cn1RV8q9dtug"
   },
   "outputs": [],
   "source": [
    "test_mse_score, test_mae_score = model_6.evaluate(X_test, Y6_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKG_B_a-o1Vz"
   },
   "source": [
    "**Save the model_6**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZorzcR3yjyuX"
   },
   "outputs": [],
   "source": [
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model should be saved to HDF5.\n",
    "\n",
    "model_6.save('/content/drive/MyDrive/Farhan/New Models/model_6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JaNX6shOjPto"
   },
   "outputs": [],
   "source": [
    "del model_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOOCGkuaekdN"
   },
   "source": [
    "#**Model no 07**\n",
    "#**K-fold validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zvSivel1plW"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y7_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y7_train = np.concatenate(\n",
    "      [Y7_train[:i * num_val_samples],\n",
    "      Y7_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_7 = build_model()\n",
    "  print(\"shapes of x patial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y7_train.shape)\n",
    "  model_7.fit(partial_X_train, partial_Y7_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_7.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enrXy15J1rAS"
   },
   "outputs": [],
   "source": [
    "print(all_scores)\n",
    "print(np.mean(all_scores))\n",
    "print(\"val_mse :\" , val_mse)\n",
    "print(\"val_mae :\" , val_mae)\n",
    "test_mse_score, test_mae_score = model_7.evaluate(X_test, Y7_test)\n",
    "print(\"test_mse_score :\" , test_mse_score)\n",
    "print(\"test_mae_score :\" , test_mae_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hCBSOrZPvq6I"
   },
   "source": [
    "**Saving the validation logs at each fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "csAgxXqj1rvc"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y7_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  partial_Y7_train = np.concatenate(\n",
    "      [Y7_train[:i * num_val_samples],\n",
    "       Y7_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  model_7 = build_model()\n",
    "  history = model_7.fit(partial_X_train, partial_Y7_train,\n",
    "                      validation_data=(val_data, val_targets),\n",
    "                      epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  mae_history = history.history['mae']\n",
    "  all_mae_histories.append(mae_history)\n",
    "\n",
    "# You can then compute the average of the per-epoch MAE scores for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7EK76nqT1xRM"
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyuLO7tp1t9d"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_mae_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Wag3d-3wdnZ"
   },
   "source": [
    "**Plotting using Matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4lb1TgR15rc"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tbFa2znawlod"
   },
   "source": [
    "**Training the final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_JUiDvF17QF"
   },
   "outputs": [],
   "source": [
    "model_7 = build_model()\n",
    "model_7.fit(X_train, Y7_train, epochs=100, batch_size=16, verbose=1)\n",
    "test_mse_score, test_mae_score = model_7.evaluate(X_test, Y7_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRqFm8-dvfAo"
   },
   "source": [
    "**Training the final model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGZwvtv5o2d7"
   },
   "source": [
    "**Save the model_7**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feFI4wFro3Ar"
   },
   "outputs": [],
   "source": [
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model should be saved to HDF5.\n",
    "\n",
    "model_7.save('/content/drive/MyDrive/Farhan/New Models/model_7.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CatsBXtx1GJm"
   },
   "outputs": [],
   "source": [
    "del model_7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LbcXv9DemV0"
   },
   "source": [
    "#**Model no 08**\n",
    "#**K-fold validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2tuxLJ9ep88"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "k=4\n",
    "num_val_samples = len(X_train) // k\n",
    "num_epochs = 100\n",
    "all_scores = []\n",
    "\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y8_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "       axis=0)\n",
    "  \n",
    "  partial_Y8_train = np.concatenate(\n",
    "      [Y8_train[:i * num_val_samples],\n",
    "      Y8_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  model_8 = build_model()\n",
    "  print(\"shapes of x patial\" ,partial_X_train.shape)\n",
    "  print(\"shapes of y partial\",partial_Y8_train.shape)\n",
    "  model_8.fit(partial_X_train, partial_Y8_train,epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  val_mse, val_mae = model_8.evaluate(val_data, val_targets, verbose=0)\n",
    "  all_scores.append(val_mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7TnKw5xxE9U"
   },
   "source": [
    "**Saving the validation logs at each fold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQs9vaL9Q6NH"
   },
   "outputs": [],
   "source": [
    "print(all_scores)\n",
    "print(np.mean(all_scores))\n",
    "print(\"val_mse :\" , val_mse)\n",
    "print(\"val_mae :\" , val_mae)\n",
    "test_mse_score, test_mae_score = model_8.evaluate(X_test, Y8_test)\n",
    "print(\"test_mse_score :\" , test_mse_score)\n",
    "print(\"test_mae_score :\" , test_mae_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q1iF6wd1wCOQ"
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "all_mae_histories = []\n",
    "for i in range(k):\n",
    "  print('processing fold #', i)\n",
    "  val_data = X_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "  val_targets = Y8_train[i * num_val_samples: (i + 1) * num_val_samples]\n",
    "\n",
    "  partial_X_train = np.concatenate(\n",
    "      [X_train[:i * num_val_samples],\n",
    "       X_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  \n",
    "  partial_Y8_train = np.concatenate(\n",
    "      [Y8_train[:i * num_val_samples],\n",
    "       Y8_train[(i + 1) * num_val_samples:]],\n",
    "      axis=0)\n",
    "  model_8 = build_model()\n",
    "  history = model_8.fit(partial_X_train, partial_Y8_train,\n",
    "                      validation_data=(val_data, val_targets),\n",
    "                      epochs=num_epochs, batch_size=1, verbose=0)\n",
    "  mae_history = history.history['mae']\n",
    "  all_mae_histories.append(mae_history)\n",
    "\n",
    "# You can then compute the average of the per-epoch MAE scores for all folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJo0cHzewwSn"
   },
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRNFJUxbwyO3"
   },
   "outputs": [],
   "source": [
    "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]\n",
    "average_mae_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwSLZH4-w0WO"
   },
   "source": [
    "**Plotting using Matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_vS12WEw4d1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(average_mae_history)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gvuq_PO1w68m"
   },
   "source": [
    "**Training the final model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZenWDg5w9HM"
   },
   "outputs": [],
   "source": [
    "model_8 = build_model()\n",
    "model_8.fit(X_train, Y8_train, epochs=10, batch_size=16, verbose=1)\n",
    "test_mse_score, test_mae_score = model_8.evaluate(X_test, Y8_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jegc5_cso354"
   },
   "source": [
    "**Save the model_8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNC6qk2ro4f7"
   },
   "outputs": [],
   "source": [
    "# Save the entire model to a HDF5 file.\n",
    "# The '.h5' extension indicates that the model should be saved to HDF5.\n",
    "\n",
    "model_8.save('/content/drive/MyDrive/Farhan/New Models/model_8.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egezx_sQpBUd"
   },
   "source": [
    "#**Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJVM_ze3pEsT"
   },
   "outputs": [],
   "source": [
    "# iterate through (/content/drive/MyDrive/Farhan) and load all h5 files and predict on test_x append to a list\n",
    "import os\n",
    "import tensorflow as tf\n",
    "#from keras import models\n",
    "\n",
    "ll = []\n",
    "data = pd.DataFrame()\n",
    "for i in os.listdir(\"/content/drive/MyDrive/Models/New Models\"):\n",
    "    if i.endswith(\".h5\"):\n",
    "        model = tf.keras.models.load_model(\"/content/drive/MyDrive/Models/New Models/\"+i)\n",
    "        y_pred = model.predict(x)\n",
    "        ll.append(np.ceil(y_pred))\n",
    "        # append ll to dataframe\n",
    "        data = data.append(pd.Series(np.ceil([x[0] for x in y_pred])), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qqDt4HMlpJ11"
   },
   "outputs": [],
   "source": [
    "data = data.T\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJaTimUxpLdw"
   },
   "outputs": [],
   "source": [
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5If3w2FMpMyq"
   },
   "outputs": [],
   "source": [
    "y_true[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgQvfOGMpOWz"
   },
   "outputs": [],
   "source": [
    "data[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lnOzdLqWpQ30"
   },
   "source": [
    "#**Displaying the results/ Visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WvdDqll3pSkD"
   },
   "outputs": [],
   "source": [
    "for i,j in enumerate(y_true[0]):\n",
    "  print(i)\n",
    "  y = y_true[:,i].reshape(200,)\n",
    "  z = data[:,i].reshape(200,)\n",
    "  coef = np.polyfit(y, z,deg= 1)\n",
    "  print(coef.shape)\n",
    "  poly1d_fn = np.poly1d(coef) \n",
    "  # poly1d_fn is now a function which takes in x and returns an estimate for y\n",
    "  plt.figure()\n",
    "  plt.plot(y, z, 'yo', y, poly1d_fn(y), '--k')\n",
    "  # plt.title(\"_\")\n",
    "  plt.xlabel('y_True_' + str(i +1) )\n",
    "  plt.ylabel('Predictions_' +str(i +1)  )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "JfmYj8HkiXpf",
    "1QtkqrPYiiql",
    "7WefUXbflj-3",
    "NPP1MflwzB2H",
    "cnndm-Fljd7i",
    "HDIkKO_RrI94",
    "DaRSNXCUjwGy",
    "t-CFl89G3TJ1",
    "cliQBIFj3Xx9",
    "qb9li3Q63ZBV",
    "-vS9naPH3aMv",
    "X1lVsh1F3bZ9",
    "ba18gVLM3cn_",
    "OXU3_SYB3d41",
    "ZxAmTUvaeAs3",
    "Ft32NhSYeFk_",
    "xVJ6jEAueLtO",
    "3XRLWW4febLm",
    "pfgDdgx6edx8",
    "uMf1ckV4ehbu",
    "tOOCGkuaekdN",
    "4LbcXv9DemV0",
    "egezx_sQpBUd",
    "lnOzdLqWpQ30"
   ],
   "machine_shape": "hm",
   "name": "nabdi Training,Prediction and Visualization.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
